DATASET IMDB (linha 2)

    A biblioteca TensorFlow possui um conjunto de datasets pré-carregados, incluindo o IMDB (Internet Movie Database),
    que contém avaliações de filmes classificadas como positivas ou negativas.

    O conjunto de dados IMDB é dividido em treinamento e teste, cada um com 25.000 avaliações de filmes.
    As avaliações estão codificadas como sequências de índices de palavras, onde cada índice representa uma
    palavra específica no dicionário do IMDB. As palavras mais frequentes têm índices menores. Cada avaliação (sequência de índices)
    é acompanhada por um rótulo de classificação binária, que indica se a avaliação é positiva (1) ou negativa (0).

# Carrega o dataset IMDB (linhas 6 a 11)

    max_features = 5000: Define o número máximo de palavras a serem consideradas no vocabulário.
    maxlen = 400: Define o comprimento máximo das sequências de palavras. Todas as sequências que tiverem mais de 400 palavras serão truncadas, e as que tiverem menos serão preenchidas com zeros.
    batch_size = 32: Define o tamanho do lote usado durante o treinamento do modelo.
    load_data: Usada para carregar os dados de treinamento e teste do conjunto IMDB.
    num_words: Definida como max_features, o que significa que apenas as max_features palavras mais frequentes serão mantidas nos dados.
    x_train: (sequências de palavras de treinamento)
    y_train: (rótulos de treinamento)
    x_test: (sequências de palavras de teste)
    y_test: (rótulos de teste).
    x_train e x_test: Aplicam a padronização nas sequências de palavras
    pad_sequences: Usada para garantir que todas as sequências tenham o mesmo comprimento (maxlen), preenchendo as sequências mais curtas com zeros no início ou truncando as sequências mais longas.

# Obtém o índice de palavras do dataset (linhas 13 a 18)

    word_index = imdb.get_word_index(): get_word_index() é uma função do conjunto de dados IMDB para obter o dicionário word_index. Esse dicionário contém as palavras do IMDB como chaves e seus índices correspondentes como valores.
    word_index = {k: (v + 3) for k, v in word_index.items()}: Atualiza o dicionário word_index adicionando um deslocamento de 3 a todos os valores (índices) no dicionário
    word_index["<PAD>"] = 0: Preenche as sequências de palavras com zeros, garantindo que todas as sequências tenham o mesmo comprimento.
    word_index["<START>"] = 1: Marca o início de uma sequência de palavras.
    word_index["<UNK>"] = 2: Representa palavras desconhecidas ou fora do vocabulário.

# Inverte o índice de palavras (linha 21)

    index_to_word = {v: k for k, v in word_index.items()}: Usa uma compreensão de dicionário para iterar sobre os itens do dicionário word_index e criar um novo dicionário index_to_word
        k = chaves (palavras) no dicionário word_index.
        v = valores (índices) no dicionário word_index.
        A compreensão de dicionário inverte as posições de k e v, onde v se torna a chave e k se torna o valor no novo dicionário index_to_word.
        Assim, cada par chave-valor do dicionário word_index é invertido no dicionário index_to_word, onde os índices são as chaves e as palavras são os valores.

# Define o modelo de rede neural (linhas 24 a 33)

    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(max_features, 64),: Camada de incorporação. Recebe as sequências de índices de palavras e converte cada índice em um vetor denso de tamanho 64. A dimensão max_features define o tamanho do vocabulário de entrada.
        tf.keras.layers.Dropout(0.5),: Camada de dropout aplica uma técnica de regularização, desligando aleatoriamente 50% dos neurônios de entrada durante o treinamento. Isso ajuda a prevenir o overfitting do modelo.
        tf.keras.layers.Conv1D(64, 5, activation='relu'),: Camada de convolução unidimensional. Ela usa filtros convolucionais para extrair características das sequências de palavras. Possui 64 filtros com tamanho de janela 5 e a função de ativação ReLU é aplicada após a convolução.
        tf.keras.layers.MaxPooling1D(pool_size=4),: Camada de pooling realiza o downsample das características extraídas pela camada de convolução. Reduz a dimensão das características, retendo apenas as informações mais relevantes. O parâmetro pool_size=4 indica que será feito um pooling de 4 elementos consecutivos.
        tf.keras.layers.LSTM(64),: Unidade recorrente de memória de longo prazo (LSTM). Ela processa a sequência de palavras e captura dependências de longo prazo. Possui 64 unidades LSTM.
        tf.keras.layers.Dense(1, activation='sigmoid'): Camada densamente conectada que possui um único neurônio de saída. Ela usa a função de ativação sigmoid para produzir uma saída entre 0 e 1
    ])

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        loss='binary_crossentropy': Argumento que define a função de perda a ser usada durante o treinamento.
        optimizer='adam': Argumento que define o otimizador a ser usado para ajustar os pesos do modelo durante o treinamento
        metrics=['accuracy']: Argumento que define as métricas a serem avaliadas durante o treinamento e teste do modelo

# Treina o modelo (linha 36)

    model.fit(x_train, y_train, batch_size=batch_size, epochs=5, validation_data=(x_test, y_test))
        x_train: Dados de treinamento de entrada, sequências de palavras processadas e padronizadas.
        y_train: Rótulos correspondentes aos dados de treinamento, indicam a classe de cada sequência de palavras..
        batch_size: Tamanho do lote a ser usado durante o treinamento. Isso define quantas amostras são processadas antes de atualizar os pesos do modelo.
        epochs=5: Número de épocas de treinamento, número de vezes que o modelo percorrerá todos os dados de treinamento.
        validation_data=(x_test, y_test): Dados de validação a serem usados durante o treinamento para avaliar o desempenho do modelo em dados não vistos anteriormente.
        x_test: Sequências de palavras de teste
        y_test: Rótulos correspondentes.

# Salvar os pesos do modelo (linha 39)

    model.save_weights('modelo_pesos.h5'):
        Contêm os parâmetros ajustados durante o treinamento, que capturam o conhecimento aprendido pelo modelo.
        Formato HDF5 é um formato de arquivo popular para armazenar e compartilhar dados científicos e de aprendizado de máquina.

# Salvar a arquitetura do modelo em formato JSON (linhas 42 a 43)

    with open('modelo_arquitetura.json', 'w') as json_file:
        json_file.write(model.to_json())

    O arquivo JSON resultante conterá informações sobre a arquitetura do modelo, como o tipo e os parâmetros de cada camada, mas não incluirá os pesos do modelo

# Salvar o índice de palavras (linhas 46 a 49)

    import json

    with open('word_index.json', 'w') as json_file:
        json.dump(word_index, json_file)

    O arquivo 'word_index.json' conterá o índice de palavras no formato JSON. Esse arquivo pode ser posteriormente carregado para obter o dicionário word_index com as informações de mapeamento entre palavras e índices.

# Salvar o índice inverso de palavras (linhas 52 a 53)

    with open('index_to_word.json', 'w') as json_file:
        json.dump(index_to_word, json_file)

    O arquivo 'index_to_word.json' conterá o índice inverso de palavras no formato JSON. Esse arquivo pode ser posteriormente carregado para obter o dicionário index_to_word com as informações de mapeamento entre índices e palavras.